1.1 文本分词
包含分词、词性标注和语义标注，词性标注即标注句子中的词是属于名词、动词等等，语义标注类似于句法分析，即标注出句子中的主谓宾结构和修饰关系。
1.2 字符串相似度
这里的字符串相似度，并不是自然语言语义上的词语相似度，而是以字母（或单个汉字）为单位，计算字符串之间的相似度。阿里的PAI平台支持如下算法：
Levenshtein Distance，即编辑距离。 一个字符串可以通过“增加一个字符”，“删除一个字符”，“替换一个字符”，从而得到另一个字符串，假设我们从字符串A转换为字符串B，前面3种操作所执行的最少次数就是A和B的相似度。求该最小次数。
Longest Common SubString，即最长公共子串。
String Subsequence Kernel，参考http://www.jmlr.org/papers/volume2/lodhi02a/lodhi02a.pdf
Cosine， 参考http://www.ics.uci.edu/~welling/teatimetalks/kernelclub04/spectrum.pdf
simhash_hamming， simhash是将一个文档，转换成一个64位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是<n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。二进制串A 和 二进制串B 的hamming距离 就是 A xor B 后二进制中1的个数。
1.3 停用词过滤
停用词过滤，是文本分析中一个预处理方法。它的功能是过滤分词结果中的噪声（例如：的、是、啊等）。
1.4 Ngram-count
ngram-count是语言模型中的其中一个步骤，完成的是基于词的基础上生成n-gram，并统计在全部语料集上，对应n-gram的个数。生成的是全局的个数，并不是单个文档的个数（输出N元词及该N元词在整个训练预料中的出现频率）。 可以参考ngram-count
参考：http://blog.csdn.net/zhoubl668/article/details/7759042
1.5 文本摘要
所谓自动文摘就是利用计算机自动地从原始文献中提取文摘，文摘是全面准确地反映某一文献中心内容地简单连贯的短文。本算法基于TextRank，通过提取文档中已存在的句子形成摘要。
具体算法原理可以参考文章TextRank: Bringing Order into Texts
利用Textrank做文本摘要的核心思想很简单，和著名的网页排名算法PageRank类似：每个句子可以作为一个网络中的节点（称为节点i），与之相连的其他节点（例如节点j）会对其重要度产生一定的“贡献值”，该“贡献值”与节点j自身的重要度以及i、j之间的相似度（也可以称为连接的强度）有关，只需要对整个图进行迭代直至收敛，最后各节点的分值即是该句子的重要性，根据重要性排序后选取前k个句子即可作为摘要。
参考：http://jayveehe.github.io/2016/05/11/da_textrank/
1.6 关键词抽取
从文本里面把跟这篇文章意义最相关的一些词抽取出来。
1.7 句子拆分
将一段文本，按标点进行句子拆分。
1.8 语义向量距离
基于算法语义向量结果（如word2vec生成的词向量），计算给定的词（或者句子）的扩展词（或者句子），即计算其中某一向量距离最近的向量集合。其中一个用法是：基于word2vec生成的词向量结果，根据输入的词返回最为相似的词列表。
1.9 Doc2Vec
将文章向量化。存在多种实现方式，包括以Word2vec为基础生成Doc2vec，也可以使用LDA等方式。
1.10 CRF条件随机场
条件随机场（conditional random field, CRF）是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场。条件随机场可以用于不同的预测问题，主要应用到标注问题中，其中线性链（linear chain）条件随机场是最典型的。
    CRF的应用场景包括：训练时间识别工具、训练命名实体识别工具、训练分词器、训练词性标注器等等。需要相应的训练集数据支持。
1.11 文章相似度
文章相似度是在字符串相似度的基础上，基于词，计算两两文章或者句子之间的相似度，文章或者句子需要以空格分割的文本，计算方式和字符串相似度类似。 相似度类型同字符串相似度
1.12 互信息
互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。本算法统计若干文章中所有词的共现情况，计算两两之间的PMI（point mutual information）。互信息定义为：
PMI(x,y)=ln(p(x,y)/(p(x)p(y)))=ln(#(x,y)D/(#x#y))
其中，#(x,y)为pair(x,y)的count数，D为pair的总数。若x、y在同一个窗口出现，那么#x+=1; #y+=1;#(x,y)+=1。
1.13 TF-IDF
TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。
1.14 PLDA
主题模型，返回文档对应的主题 LDA(Latent Dirichlet allocation)，是一种主题模型，它可以将文档集中每篇文档的主题按照概率分布的形式给出。同时它是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。
1.15 Word2Vec
Word2Vec是Google在2013年开源的一个将词表转为向量的算法，其利用神经网络，可以通过训练，将词映射到K维度空间向量，甚至对于表示词的向量进行操作还能和语义相对应。
